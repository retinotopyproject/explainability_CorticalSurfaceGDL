import numpy as np
import torch
import os
import os.path as osp

from Retinotopy.functions.def_ROIs_WangParcels import roi as roi2
from Retinotopy.functions.def_ROIs_WangParcelsPlusFovea import roi
from Retinotopy.functions.plusFovea import add_fovea
from Retinotopy.functions.plusFovea import add_fovea_R
from Retinotopy.functions.error_metrics import smallest_angle

"""
This code was copied from the deepRetinotopy repository, from the file 
'ModelEval_MeanDeltaTheta_PA.py' in the Manuscript/stats/left_hemi dir
(https://github.com/Puckett-Lab/deepRetinotopy/). The file has been
modified so that it can be used for either hemisphere, depending on 
the value specified in the var 'hemisphere', and for either the HCP or NYU 
dataset. It can be used with either finetuned or non-finetuned models
using the NYU dataset. References to Benson14 predictions have been removed, 
as they were not used.
This file can be run to create various 'ErrorPerParticipant' .npz files
for the PA LH/RH Test set, which are used in 
Figure7b_DeltaTheta_ModelvsAverageMap.py.

The code requires that several other files are generated first:
    - Manuscript/plots/output/{train_mean_filename}MaskEccentricity_above1below8ecc_{L or R}H.npz
    (generated by running Manuscript/plots/{HCP or NYU}SuppFigure3_ECCaverage_plot.py
    for the corresponding hemisphere and model)
    - Manuscript/plots/output/{train_mean_filename}AveragePolarAngleMap_{L or R}H.npz 
    (generated by running Manuscript/plots/{HCP or NYU}SuppFigure3_PAaverage_plot.py 
    for the corresponding hemisphere and model)
    - The PA Test set results for the best performing model. This can be used
    for HCP or NYU models saved in Models/generalizability. 
    The best perfoming model (of models 1-5) is specified with the 
    'selected_model' var.

Note: code implementation assumes that the file is being run from the dir 
Manuscript/stats - I have modified the code to automatically set the 
working dir to this (if it isn't already).
"""
# Set the working directory to Manuscript/stats
os.chdir(os.path.dirname(os.path.realpath(__file__)))


# Number of test set participants in HCP dataset
HCP_TEST_SET_SIZE = int(10)
# Total number of participants in NYU dataset
NYU_N_EXAMPLES = int(43)
# Total number of cortical nodes in the mesh
NUMBER_CORTICAL_NODES = int(64984)
# Number of nodes within each hemisphere
NUMBER_HEMI_NODES = int(NUMBER_CORTICAL_NODES / 2)


def PA_difference(model, hemisphere='Left', selected_model=1, dataset='HCP', 
                    n_finetuning_subj=None, num_epochs=20):
    """Function to determine the difference between empirical
    and predicted polar angle values for higher order visual areas, early
    visual cortex and dorsal V1-3.

    Args:
        model (str): 'deepRetinotopy' or 'average'
        hemisphere (str): For which hemisphere will files be generated?
                          ('Left' or 'Right')
        selected_model (int): For which model (models 1-5) will files be
                              generated?
        dataset (str): For which of the datasets used will files be generated?
                       ('HCP' or 'NYU')
        n_finetuning_subj (str or None): How many participants were 
                           allocated to a 'Training' set for finetuning?
                           If n_finetuning_subj == None, finetuning 
                           was not performed. This var is ignored if
                           dataset == 'HCP'.
        num_epochs (int): How many epochs did finetuning occur for? If 
                          n_finetuning_subj == None or dataset == 'HCP',
                          the var is ignored (as finetuning didn't take place).

    Output: .npz files in ./../output named:
    '{dataset_filename}ErrorPerParticipant_PA_{L or R}H_model{str(selected_model)}_WangParcels_' + 
        str(model) + '_1-8.npz'
    '{dataset_filename}ErrorPerParticipant_PA_{L or R}H_model{str(selected_model)}_EarlyVisualCortex_' + 
        str(model) + '_1-8.npz'
    '{dataset_filename}ErrorPerParticipant_PA_{L or R}H_model{str(selected_model)}_dorsalV1-3_' + 
        str(model) + '_1-8.npz'

    """
    # Create the file name components and test set size for the given params
    hemi_filename, dataset_filename, train_mean_filename, testset_size, \
        testset_results_dir = set_filenames_and_testset_size(hemisphere=hemisphere,
                            dataset=dataset, n_finetuning_subj=n_finetuning_subj, 
                            num_epochs=num_epochs)
        
    visual_areas = [
        ['hV4', 'VO1', 'VO2', 'PHC1', 'PHC2', 'V3a', 'V3b', 'LO1', 'LO2',
         'TO1',
         'TO2', 'IPS0', 'IPS1', 'IPS2', 'IPS3', 'IPS4', 'IPS5', 'SPL1']]
    eccentricity_mask = np.reshape(
        np.load(f'./../plots/output/{train_mean_filename}MaskEccentricity_' +
                f'above1below8ecc_{hemi_filename}H.npz')['list'], (-1))
    # Average map
    PA_average = np.load(f'./../plots/output/{train_mean_filename}AveragePolarAngleMap_' +
                        f'{hemi_filename}H.npz')['list']

    for k in range(len(visual_areas)):
        mean_delta = []
        predictions = torch.load(osp.join('./../..', 'Models', 'generalizability', 
        testset_results_dir, f'{dataset_filename}-intactData_PA_' + 
        f'{hemi_filename}H_model{str(selected_model)}.pt'), map_location='cpu')

        # ROI settings
        label_primary_visual_areas = ['ROI']
        final_mask_L, final_mask_R, index_L_mask, index_R_mask = roi(
            label_primary_visual_areas)
        ROI1 = np.zeros((NUMBER_HEMI_NODES, 1))
        # Apply relevant hemisphere's mask to ROI1
        if hemisphere == 'Left':
            ROI1[final_mask_L == 1] = 1
        else:
            # hemisphere == 'Right'
            ROI1[final_mask_R == 1] = 1

        # Visual areas
        final_mask_L, final_mask_R, index_L_mask, index_R_mask = roi2(
            visual_areas[k])
        primary_visual_areas = np.zeros((NUMBER_HEMI_NODES, 1))
        # Apply relevant hemisphere's mask to primary visual areas
        if hemisphere == 'Left':
            primary_visual_areas[final_mask_L == 1] = 1
        else:
            # hemisphere == 'Right'
            primary_visual_areas[final_mask_R == 1] = 1

        # Final mask
        mask = ROI1 + primary_visual_areas
        mask = mask[ROI1 == 1]

        theta_withinsubj = []
        for j in range(len(predictions['Predicted_values'])):
            for i in range(len(predictions['Predicted_values'])):
                if i == j:
                    # Loading predicted values
                    # Polar angles
                    if model == 'deepRetinotopy':
                        pred = np.reshape(
                            np.array(predictions['Predicted_values'][i]),
                            (-1, 1))
                    if model == 'average':
                        pred = np.reshape(np.array(PA_average), (-1, 1))
                    measured = np.reshape(
                        np.array(predictions['Measured_values'][j]),
                        (-1, 1))

                    # Rescaling
                    minus = pred > 180
                    sum = pred < 180
                    pred[minus] = pred[minus] - 180
                    pred[sum] = pred[sum] + 180
                    pred = np.array(pred) * (np.pi / 180)

                    minus = measured > 180
                    sum = measured < 180
                    measured[minus] = measured[minus] - 180
                    measured[sum] = measured[sum] + 180
                    measured = np.array(measured) * (np.pi / 180)

                    # Computing delta theta
                    theta = smallest_angle(pred[eccentricity_mask],
                                            measured[eccentricity_mask])
                    theta_withinsubj.append(
                        theta[mask[eccentricity_mask] > 1])
        mean_theta_withinsubj = np.mean(np.array(theta_withinsubj), axis=1)
        mean_delta.append(mean_theta_withinsubj)
        mean_delta = np.reshape(np.array(mean_delta), (1, -1))

    # Remove the substring 'testset' from the dataset filename
    output_filename = dataset_filename.replace('testset', '')
    # Add a trailing '_' if required
    if output_filename != '':
        output_filename  += '_'
    # Remove any additional double '_' chars
    output_filename = output_filename.replace('__', '_')
    # Save the error per participant
    np.savez(f'./output/{output_filename}ErrorPerParticipant_PA_{hemi_filename}H_' +
             f'model{str(selected_model)}_WangParcels_{str(model)}_1-8.npz',
             list=np.reshape(theta_withinsubj, (testset_size, -1)))

    # Early visual cortex
    label_primary_visual_areas = ['V1d', 'V1v', 'fovea_V1', 'V2d', 'V2v',
                                  'fovea_V2', 'V3d', 'V3v', 'fovea_V3']
    # Add fovea to the relevant hemisphere's V1, V2, V3 parcels
    if hemisphere == 'Left':
        V1, V2, V3 = add_fovea(label_primary_visual_areas)
    else:
        # hemisphere == 'Right'
        V1, V2, V3 = add_fovea_R(label_primary_visual_areas)
    primary_visual_areas = np.sum(
        [np.reshape(V1, (-1, 1)), np.reshape(V2, (-1, 1)),
         np.reshape(V3, (-1, 1))], axis=0)

    mean_delta_2 = []
    predictions = torch.load(osp.join('./../..', 'Models', 'generalizability', 
    testset_results_dir, f'{dataset_filename}-intactData_PA_' + 
    f'{hemi_filename}H_model{str(selected_model)}.pt'), map_location='cpu')

    # ROI settings
    label_primary_visual_areas = ['ROI']
    final_mask_L, final_mask_R, index_L_mask, index_R_mask = roi(
        label_primary_visual_areas)
    ROI1 = np.zeros((NUMBER_HEMI_NODES, 1))
    # Apply relevant hemisphere's mask to ROI1
    if hemisphere == 'Left':
        ROI1[final_mask_L == 1] = 1
    else:
        # hemisphere == 'Right'
        ROI1[final_mask_R == 1] = 1

    mask = ROI1 + np.reshape(primary_visual_areas, (NUMBER_HEMI_NODES, 1))
    mask = mask[ROI1 == 1]

    theta_withinsubj = []
    for j in range(len(predictions['Predicted_values'])):
        for i in range(len(predictions['Predicted_values'])):
            if i == j:
                # Polar angle
                if model == 'deepRetinotopy':
                    pred = np.reshape(
                        np.array(predictions['Predicted_values'][i]),
                        (-1, 1))
                if model == 'average':
                    pred = np.reshape(np.array(PA_average), (-1, 1))
                measured = np.reshape(
                    np.array(predictions['Measured_values'][j]),
                    (-1, 1))

                # Rescaling
                minus = pred > 180
                sum = pred < 180
                pred[minus] = pred[minus] - 180
                pred[sum] = pred[sum] + 180
                pred = np.array(pred) * (np.pi / 180)

                minus = measured > 180
                sum = measured < 180
                measured[minus] = measured[minus] - 180
                measured[sum] = measured[sum] + 180
                measured = np.array(measured) * (np.pi / 180)

                # Computing delta theta
                theta = smallest_angle(pred[eccentricity_mask],
                                        measured[eccentricity_mask])
                theta_withinsubj.append(theta[mask[eccentricity_mask] > 1])
    mean_theta_withinsubj = np.mean(np.array(theta_withinsubj), axis=1)
    mean_delta_2.append(mean_theta_withinsubj)
    mean_delta_2 = np.reshape(np.array(mean_delta_2), (1, -1))

    # Remove the substring 'testset' from the dataset filename
    output_filename = dataset_filename.replace('testset', '')
    # Add a trailing '_' if required
    if output_filename != '':
        output_filename  += '_'
    # Remove any additional double '_' chars
    output_filename = output_filename.replace('__', '_')
    # Save the error per participant
    np.savez(f'./output/{output_filename}ErrorPerParticipant_PA_{hemi_filename}H_' +
            f'model{str(selected_model)}_EarlyVisualCortex_{str(model)}_1-8.npz', 
            list=np.reshape(theta_withinsubj, (testset_size, -1)))

    # Dorsal early visual cortex
    visual_areas = [
        ['V1d', 'V2d', 'V3d']]
    for k in range(len(visual_areas)):
        mean_delta = []
        predictions = torch.load(osp.join('./../..', 'Models', 'generalizability', 
        testset_results_dir, f'{dataset_filename}-intactData_PA_' + 
        f'{hemi_filename}H_model{str(selected_model)}.pt'), map_location='cpu')

        # ROI settings
        label_primary_visual_areas = ['ROI']
        final_mask_L, final_mask_R, index_L_mask, index_R_mask = roi(
            label_primary_visual_areas)
        ROI1 = np.zeros((NUMBER_HEMI_NODES, 1))
        # Apply relevant hemisphere's mask to ROI1
        if hemisphere == 'Left':
            ROI1[final_mask_L == 1] = 1
        else:
            # hemisphere == 'Right'
            ROI1[final_mask_R == 1] = 1

        # Visual areas
        final_mask_L, final_mask_R, index_L_mask, index_R_mask = roi2(
            visual_areas[k])
        primary_visual_areas = np.zeros((NUMBER_HEMI_NODES, 1))
        # Apply relevant hemisphere's mask to primary visual areas
        if hemisphere == 'Left':
            primary_visual_areas[final_mask_L == 1] = 1
        else:
            # hemisphere == 'Right'
            primary_visual_areas[final_mask_R == 1] = 1

        # Final mask
        mask = ROI1 + primary_visual_areas
        mask = mask[ROI1 == 1]

        theta_withinsubj = []
        for j in range(len(predictions['Predicted_values'])):
            for i in range(len(predictions['Predicted_values'])):

                if i == j:
                    # Polar angle
                    if model == 'deepRetinotopy':
                        pred = np.reshape(
                            np.array(predictions['Predicted_values'][i]),
                            (-1, 1))
                    if model == 'average':
                        pred = np.reshape(np.array(PA_average), (-1, 1))
                    measured = np.reshape(
                        np.array(predictions['Measured_values'][j]),
                        (-1, 1))

                    # Rescaling
                    minus = pred > 180
                    sum = pred < 180
                    pred[minus] = pred[minus] - 180
                    pred[sum] = pred[sum] + 180
                    pred = np.array(pred) * (np.pi / 180)

                    minus = measured > 180
                    sum = measured < 180
                    measured[minus] = measured[minus] - 180
                    measured[sum] = measured[sum] + 180
                    measured = np.array(measured) * (np.pi / 180)

                    # Computing delta theta
                    theta = smallest_angle(pred[eccentricity_mask],
                                            measured[eccentricity_mask])
                    theta_withinsubj.append(
                        theta[mask[eccentricity_mask] > 1])

        mean_theta_withinsubj = np.mean(np.array(theta_withinsubj), axis=1)
        mean_delta.append(mean_theta_withinsubj)
        mean_delta = np.reshape(np.array(mean_delta), (1, -1))

    # Remove the substring 'testset' from the dataset filename
    output_filename = dataset_filename.replace('testset', '')
    # Add a trailing '_' if required
    if output_filename != '':
        output_filename  += '_'
    # Remove any additional double '_' chars
    output_filename = output_filename.replace('__', '_')
    # Save the error per participant
    np.savez(f'./output/{output_filename}ErrorPerParticipant_PA_{hemi_filename}H_' +
            f'model{str(selected_model)}_dorsalV1-3_{str(model)}_1-8.npz', 
            list=np.reshape(theta_withinsubj, (testset_size, -1)))


def set_filenames_and_testset_size(hemisphere, dataset, n_finetuning_subj,
                                    num_epochs):
    """
    Helper method, used to assign file name components (hemisphere file name,
    test set filename, test set directory) and the size of the test set for the 
    given dataset.
    Args:
        hemisphere (str): For which hemisphere will files be generated?
                          ('Left' or 'Right')
        dataset (str): For which of the datasets used will files be generated?
                          ('HCP' or 'NYU')
        n_finetuning_subj (str or None): How many participants were 
                           allocated to a 'Training' set for finetuning?
                           If n_finetuning_subj == None, finetuning 
                           was not performed. This var is ignored if
                           dataset == 'HCP'.
        num_epochs (int): How many epochs did finetuning occur for? If 
                          n_finetuning_subj == None or dataset == 'HCP',
                          the var is ignored (as finetuning didn't take place).

    Output:
        hemi_filename (str): Shortened version of hemisphere string. 'L' or 'R'
        dataset_filename (str): includes the dataset name, as well as the 
                                number of finetuning participants and number
                                of finetuning epochs (if finetuning was 
                                performed)
        train_mean_filename (str): identifies the relevant average training set
                                 PA/ECC maps to be loaded by the PA_difference 
                                 method. '' if using the HCP dataset or NYU
                                 dataset with no finetuning; 'NYU_*subj' if
                                 using the NYU dataset with * subjects reserved
                                 for finetuning.
        testset_size (int): the number of participants in the test set
        testset_results_dir (str): the directory in which the test set results
                                   are saved ('testset_results', 'NYU_testset_results', 
                                   'NYU_testset_finetuned_results')

    """
    # Create the file name components for the chosen hemisphere
    hemi_filename = hemisphere[0]
    # Set the mean training set file name for HCP and NYU non-finetuning
    train_mean_filename = ''

    # Set the file names and number of test set participants (based on the dataset used)
    if dataset == 'HCP':
        dataset_filename = 'testset'
        # 10 participants in the test set
        testset_size = HCP_TEST_SET_SIZE
        # Set the name of the testset results directory
        testset_results_dir = 'testset_results'

    elif dataset == 'NYU':
        # If no finetuning:
        dataset_filename = 'NYU_testset'
        # 43 participants in the test set
        testset_size = NYU_N_EXAMPLES
        # Set the name of the testset results directory
        testset_results_dir = 'NYU_testset_results'

        # If finetuning is performed:
        if n_finetuning_subj is not None:
            # Add the number of subjects used to finetune and number of epochs
            dataset_filename += \
                f'_finetuned_{n_finetuning_subj}subj_{num_epochs}epochs'
            # Remove the # of finetuning participants from the test set size
            testset_size -= n_finetuning_subj
            # Add 'finetuned' to the testset results dir name if required
            testset_results_dir = 'NYU_testset_finetuned_results'
            # Set the mean training set file name
            train_mean_filename = f'NYU_finetuned_{n_finetuning_subj}subj_'
    
    return hemi_filename, dataset_filename, train_mean_filename, \
                testset_size, testset_results_dir


# Create an output folder if it doesn't already exist
directory = './output'
if not os.path.exists(directory):
    os.makedirs(directory)

# # Generates files for HCP models
# for hemi in ['Left', 'Right']:
#     PA_difference('average', hemi, 5, 'HCP')
#     PA_difference('deepRetinotopy', hemi, 5, 'HCP')

# # Generates files for NYU models (no finetuning)
# for hemi in ['Left', 'Right']:
#     PA_difference('average', hemi, 5, 'NYU')
#     PA_difference('deepRetinotopy', hemi, 5, 'NYU')

# # Generates files for NYU models with finetuning
# for hemi in ['Left', 'Right']:
#     # Finetuning sets with 8 and 12 participants
#     for n_subj in [8, 12]:
#         # Finetuning for 5, 10, 20 epochs
#         for epochs in [5, 10, 20]:
#             PA_difference('average', hemi, 5, 'NYU', n_subj, epochs)
#             PA_difference('deepRetinotopy', hemi, 5, 'NYU', n_subj, epochs)



